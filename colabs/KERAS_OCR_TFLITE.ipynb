{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml8RTXFZxPkd"
   },
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i3X7uUDbxCXD",
    "outputId": "9fcabe71-a03a-46a8-8129-4b4e79603012"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4o-1GNYVxWUw",
    "outputId": "77bc68ee-ec0d-4dfd-995b-ff98f1182692"
   },
   "outputs": [],
   "source": [
    "!pip install validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5Mme4MUCxVM8",
    "outputId": "0e84750a-9dc8-481d-a0a7-7574e9d294cd"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwgbqcnBxnc9"
   },
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duqhKKqbxpc9"
   },
   "outputs": [],
   "source": [
    "DEFAULT_BUILD_PARAMS = {\n",
    "    'height': 31,\n",
    "    'width': 200,\n",
    "    'color': False,\n",
    "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
    "    'rnn_units': (128, 128),\n",
    "    'dropout': 0.25,\n",
    "    'rnn_steps_to_discard': 2,\n",
    "    'pool_size': 2,\n",
    "    'stn': True,\n",
    "}\n",
    "\n",
    "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
    "\n",
    "PRETRAINED_WEIGHTS = {\n",
    "    'kurapan': {\n",
    "        'alphabet': DEFAULT_ALPHABET,\n",
    "        'build_params': DEFAULT_BUILD_PARAMS,\n",
    "        'weights': {\n",
    "            'notop': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
    "                'filename': 'crnn_kurapan_notop.h5',\n",
    "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
    "            },\n",
    "            'top': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
    "                'filename': 'crnn_kurapan.h5',\n",
    "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH9oviLCzOI6"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkP9-8YZzTWh"
   },
   "outputs": [],
   "source": [
    "def swish(x, beta=1):\n",
    "    return x * keras.backend.sigmoid(beta * x)\n",
    "\n",
    "\n",
    "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
    "\n",
    "\n",
    "def _repeat(x, num_repeats):\n",
    "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "    x = tf.reshape(x, shape=(-1, 1))\n",
    "    x = tf.matmul(x, ones)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "def _meshgrid(height, width):\n",
    "    x_linspace = tf.linspace(-1., 1., width)\n",
    "    y_linspace = tf.linspace(-1., 1., height)\n",
    "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
    "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
    "    ones = tf.ones_like(x_coordinates)\n",
    "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "    return indices_grid\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-statements\n",
    "def _transform(inputs):\n",
    "    locnet_x, locnet_y = inputs\n",
    "    output_size = locnet_x.shape[1:]\n",
    "    batch_size = tf.shape(locnet_x)[0]\n",
    "    height = tf.shape(locnet_x)[1]\n",
    "    width = tf.shape(locnet_x)[2]\n",
    "    num_channels = tf.shape(locnet_x)[3]\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
    "    locnet_y = tf.cast(locnet_y, 'float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "    indices_grid = _meshgrid(output_height, output_width)\n",
    "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
    "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
    "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
    "\n",
    "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
    "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "    x = tf.reshape(x_s, [-1])\n",
    "    y = tf.reshape(y_s, [-1])\n",
    "\n",
    "    # Interpolate\n",
    "    height_float = tf.cast(height, dtype='float32')\n",
    "    width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "\n",
    "    x = tf.cast(x, dtype='float32')\n",
    "    y = tf.cast(y, dtype='float32')\n",
    "    x = .5 * (x + 1.0) * width_float\n",
    "    y = .5 * (y + 1.0) * height_float\n",
    "\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    max_y = tf.cast(height - 1, dtype='int32')\n",
    "    max_x = tf.cast(width - 1, dtype='int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    flat_image_dimensions = width * height\n",
    "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
    "    flat_output_dimensions = output_height * output_width\n",
    "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
    "    base_y0 = base + y0 * width\n",
    "    base_y1 = base + y1 * width\n",
    "    indices_a = base_y0 + x0\n",
    "    indices_b = base_y1 + x0\n",
    "    indices_c = base_y0 + x1\n",
    "    indices_d = base_y1 + x1\n",
    "\n",
    "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
    "    flat_image = tf.cast(flat_image, dtype='float32')\n",
    "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "    transformed_image = tf.add_n([\n",
    "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
    "        area_d * pixel_values_d\n",
    "    ])\n",
    "    # Finished interpolation\n",
    "\n",
    "    transformed_image = tf.reshape(transformed_image,\n",
    "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALRK3SR1x8JH"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaRONZgExrQL"
   },
   "outputs": [],
   "source": [
    "def build_model(alphabet,\n",
    "                height,\n",
    "                width,\n",
    "                color,\n",
    "                filters,\n",
    "                rnn_units,\n",
    "                dropout,\n",
    "                rnn_steps_to_discard,\n",
    "                pool_size,\n",
    "                stn=True):\n",
    "    \"\"\"Build a Keras CRNN model for character recognition.\n",
    "    Args:\n",
    "        height: The height of cropped images\n",
    "        width: The width of cropped images\n",
    "        color: Whether the inputs should be in color (RGB)\n",
    "        filters: The number of filters to use for each of the 7 convolutional layers\n",
    "        rnn_units: The number of units for each of the RNN layers\n",
    "        dropout: The dropout to use for the final layer\n",
    "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
    "        pool_size: The size of the pooling steps\n",
    "        stn: Whether to add a Spatial Transformer layer\n",
    "    \"\"\"\n",
    "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
    "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
    "    inputs = keras.layers.Input((height, width, 3 if color else 1), name='input', batch_size=1)\n",
    "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
    "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
    "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
    "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
    "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
    "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
    "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
    "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
    "    if stn:\n",
    "        # pylint: disable=pointless-string-statement\n",
    "        \"\"\"Spatial Transformer Layer\n",
    "        Implements a spatial transformer layer as described in [1]_.\n",
    "        Borrowed from [2]_:\n",
    "        downsample_fator : float\n",
    "            A value of 1 will keep the orignal size of the image.\n",
    "            Values larger than 1 will down sample the image. Values below 1 will\n",
    "            upsample the image.\n",
    "            example image: height= 100, width = 200\n",
    "            downsample_factor = 2\n",
    "            output image will then be 50, 100\n",
    "        References\n",
    "        ----------\n",
    "        .. [1]  Spatial Transformer Networks\n",
    "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
    "                Submitted on 5 Jun 2015\n",
    "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
    "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
    "        \"\"\"\n",
    "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
    "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
    "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
    "                                       activation='relu')(stn_input_layer)\n",
    "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(6,\n",
    "                                      weights=[\n",
    "                                          np.zeros((64, 6), dtype='float32'),\n",
    "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
    "                                      ])(locnet_y)\n",
    "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
    "        x = keras.layers.Lambda(_transform,\n",
    "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
    "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
    "                                           (height // pool_size**2) * filters[-1]),\n",
    "                             name='reshape')(x)\n",
    "\n",
    "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
    "\n",
    "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_10')(x)\n",
    "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_10_back')(x)\n",
    "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
    "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_11')(rnn_1_add)\n",
    "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_11_back')(rnn_1_add)\n",
    "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
    "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
    "    x = keras.layers.Dense(len(alphabet) + 1,\n",
    "                           kernel_initializer='he_normal',\n",
    "                           activation='softmax',\n",
    "                           name='fc_12')(x)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPBjgj6gyECQ"
   },
   "outputs": [],
   "source": [
    "build_params = DEFAULT_BUILD_PARAMS\n",
    "alphabets = DEFAULT_ALPHABET\n",
    "blank_index = len(alphabets)\n",
    "\n",
    "model = build_model(alphabet=alphabets, **build_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLVl95alzwI8"
   },
   "source": [
    "## Download and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmBRQeL90HoQ"
   },
   "outputs": [],
   "source": [
    "def get_default_cache_dir():\n",
    "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
    "                                                                              '.keras-ocr')))\n",
    "def sha256sum(filename):\n",
    "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    b = bytearray(128 * 1024)\n",
    "    mv = memoryview(b)\n",
    "    with open(filename, 'rb', buffering=0) as f:\n",
    "        for n in iter(lambda: f.readinto(mv), 0):\n",
    "            h.update(mv[:n])\n",
    "    return h.hexdigest()\n",
    "\n",
    "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
    "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
    "    hash.\n",
    "    Args:\n",
    "        url: The file to download\n",
    "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
    "            matches, we don't download it again.\n",
    "        cache_dir: The directory in which to cache the file. The default is\n",
    "            `~/.keras-ocr`.\n",
    "        verbose: Whether to log progress\n",
    "        filename: The filename to use for the file. By default, the filename is\n",
    "            derived from the URL.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = get_default_cache_dir()\n",
    "    if filename is None:\n",
    "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
    "    if verbose:\n",
    "        print('Looking for ' + filepath)\n",
    "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
    "        if verbose:\n",
    "            print('Downloading ' + filepath)\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5lNpYwczLuO",
    "outputId": "2432de2d-8d4a-47c9-ae5e-08e2db125592"
   },
   "outputs": [],
   "source": [
    "weights_dict = PRETRAINED_WEIGHTS['kurapan']\n",
    "\n",
    "model.load_weights(download_and_verify(url=weights_dict['weights']['top']['url'],\n",
    "                                       filename=weights_dict['weights']['top']['filename'],\n",
    "                                       sha256=weights_dict['weights']['top']['sha256']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czQXrbwF1rWO"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUyx8EAV0IjX",
    "outputId": "1bd7ab72-d0b9-486e-99d1-d70f7778388f"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dg50iNc106w"
   },
   "source": [
    "## Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6K9tv0u1umk",
    "outputId": "d5412632-f754-443d-86ea-1856ef75c0d9"
   },
   "outputs": [],
   "source": [
    "quantization = \"float16\" #@param [\"dr\", \"int8\", \"float16\"]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if quantization == 'float16':\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "tf_lite_model = converter.convert()\n",
    "open(f'ocr_{quantization}.tflite', 'wb').write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtwNAQp72EHR"
   },
   "source": [
    "**Note** : Support for CTC Decoder is not available in TFLite yet. So while converting we removed CTCDecoder in model part. We need to run Decoder from the output of the model. \n",
    "\n",
    "Refer to this [issue](https://github.com/tensorflow/tensorflow/issues/33494) regarding CTC decoder support in TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rdJyCXo2Xzs"
   },
   "source": [
    "## TFLite Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPC_SGpy2vQX"
   },
   "source": [
    "#### CTC Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7mfepcG2pKN"
   },
   "outputs": [],
   "source": [
    "def decoder(y_pred):\n",
    "    input_shape = tf.keras.backend.shape(y_pred)\n",
    "    input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
    "        input_shape[1], 'float32')\n",
    "    unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
    "    unpadded_shape = tf.keras.backend.shape(unpadded)\n",
    "    padded = tf.pad(unpadded,\n",
    "                    paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
    "                    constant_values=-1)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQqZgvQz2yjl"
   },
   "source": [
    "#### TFLite model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZXR0VZ12DIu"
   },
   "outputs": [],
   "source": [
    "input_data = cv2.imread('/content/demo_1.png', cv2.IMREAD_GRAYSCALE)\n",
    "input_data = cv2.resize(input_data, (200, 31))\n",
    "input_data = input_data[np.newaxis]\n",
    "input_data = np.expand_dims(input_data, 3)\n",
    "input_data = input_data.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtm2RNd91-vB",
    "outputId": "e65d19d0-6f31-4673-ac2a-e91b2441f44c"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"/content/ocr_float16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape']\n",
    "print(input_shape)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfQDiPXM2tBz",
    "outputId": "8cb86134-0a45-46d2-8762-d8c124ef413f"
   },
   "outputs": [],
   "source": [
    "decoded = decoder(output)\n",
    "final_output = \"\".join(alphabets[index] for index in decoded[0] if index not in [blank_index, -1])\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjiIkzxO6oJ4"
   },
   "source": [
    "## Dynamic Range Model benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XihAxVoU6rhU"
   },
   "source": [
    "11-27 14:25:31.244 14521 14521 E tflite  : Average inference timings in us: Warmup: 244200, Init: 9208, Inference: 226278Overall max resident set size = 46.3711 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 35.834 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlrZl1GT6soU"
   },
   "source": [
    "## Float16 benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpsOjLaW6sqF"
   },
   "source": [
    "11-27 14:27:52.964 14992 14992 E tflite  : Average inference timings in us: Warmup: 768714, Init: 3519, Inference: 367842Overall max resident set size = 154.484 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 128.042 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmnnRktG28CX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KERAS_OCR_TFLITE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F3TY8FYBLTN",
    "outputId": "041f97ff-1aae-44fa-b29d-f3195d9d8862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/82/e8d0fb64df623a3b716145192ed50604f444889778b37e0e9262753d5046/onnx-1.8.0-cp36-cp36m-manylinux2010_x86_64.whl (7.7MB)\n",
      "\u001b[K     |████████████████████████████████| 7.7MB 5.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (50.3.2)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EHV8wcijq0_K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import model_urls\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "import onnx\n",
    "# import onnxruntime\n",
    "# from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BhqJRuI_A4HS",
    "outputId": "5af666c7-a42b-4662-e5ea-7111f64bd96d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l87vID4QO0yr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import model_urls\n",
    "from collections import namedtuple\n",
    "\n",
    "def init_weights(modules):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "class vgg16_bn(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze=True):\n",
    "        super(vgg16_bn, self).__init__()\n",
    "        model_urls['vgg16_bn'] = model_urls['vgg16_bn'].replace('https://', 'http://')\n",
    "        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(12):         # conv2_2\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 19):         # conv3_3\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(19, 29):         # conv4_3\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(29, 39):         # conv5_3\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        # fc6, fc7 without atrous conv\n",
    "        self.slice5 = torch.nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
    "                nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        if not pretrained:\n",
    "            init_weights(self.slice1.modules())\n",
    "            init_weights(self.slice2.modules())\n",
    "            init_weights(self.slice3.modules())\n",
    "            init_weights(self.slice4.modules())\n",
    "\n",
    "        init_weights(self.slice5.modules())        # no pretrained model for fc6 and fc7\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.slice1.parameters():      # only first conv\n",
    "                param.requires_grad= False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu3_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu5_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_fc7 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", ['fc7', 'relu5_3', 'relu4_3', 'relu3_2', 'relu2_2'])\n",
    "        out = vgg_outputs(h_fc7, h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n",
    "        return out\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : visual feature [batch_size x T x input_size]\n",
    "        output : contextual feature [batch_size x T x output_size]\n",
    "        \"\"\"\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
    "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
    "        return output\n",
    "\n",
    "class VGG_FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=256):\n",
    "        super(VGG_FeatureExtractor, self).__init__()\n",
    "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
    "                               int(output_channel / 2), output_channel]\n",
    "        self.ConvNet = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n",
    "\n",
    "class ResNet_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of FAN (http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(ResNet_FeatureExtractor, self).__init__()\n",
    "        self.ConvNet = ResNet(input_channel, output_channel, BasicBlock, [1, 2, 5, 3])\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = self._conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = self._conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def _conv3x3(self, in_planes, out_planes, stride=1):\n",
    "        \"3x3 convolution with padding\"\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                         padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.output_channel_block = [int(output_channel / 4), int(output_channel / 2), output_channel, output_channel]\n",
    "\n",
    "        self.inplanes = int(output_channel / 8)\n",
    "        self.conv0_1 = nn.Conv2d(input_channel, int(output_channel / 16),\n",
    "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn0_1 = nn.BatchNorm2d(int(output_channel / 16))\n",
    "        self.conv0_2 = nn.Conv2d(int(output_channel / 16), self.inplanes,\n",
    "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn0_2 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n",
    "        self.conv1 = nn.Conv2d(self.output_channel_block[0], self.output_channel_block[\n",
    "                               0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.output_channel_block[0])\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.layer2 = self._make_layer(block, self.output_channel_block[1], layers[1], stride=1)\n",
    "        self.conv2 = nn.Conv2d(self.output_channel_block[1], self.output_channel_block[\n",
    "                               1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.output_channel_block[1])\n",
    "\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
    "        self.layer3 = self._make_layer(block, self.output_channel_block[2], layers[2], stride=1)\n",
    "        self.conv3 = nn.Conv2d(self.output_channel_block[2], self.output_channel_block[\n",
    "                               2], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.output_channel_block[2])\n",
    "\n",
    "        self.layer4 = self._make_layer(block, self.output_channel_block[3], layers[3], stride=1)\n",
    "        self.conv4_1 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
    "                                 3], kernel_size=2, stride=(2, 1), padding=(0, 1), bias=False)\n",
    "        self.bn4_1 = nn.BatchNorm2d(self.output_channel_block[3])\n",
    "        self.conv4_2 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
    "                                 3], kernel_size=2, stride=1, padding=0, bias=False)\n",
    "        self.bn4_2 = nn.BatchNorm2d(self.output_channel_block[3])\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0_1(x)\n",
    "        x = self.bn0_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv0_2(x)\n",
    "        x = self.bn0_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.bn4_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.bn4_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdqfhOoaOMcg"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, hidden_size, num_class):\n",
    "        super(Model, self).__init__()\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "        self.FeatureExtraction = ResNet_FeatureExtractor(input_channel, output_channel)\n",
    "        self.FeatureExtraction_output = output_channel  # int(imgH/16-1) * 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        self.SequenceModeling = nn.Sequential(\n",
    "            BidirectionalLSTM(self.FeatureExtraction_output, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, hidden_size))\n",
    "        self.SequenceModeling_output = hidden_size\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        self.Prediction = nn.Linear(self.SequenceModeling_output, num_class)\n",
    "\n",
    "\n",
    "    def forward(self, input, text):\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.FeatureExtraction(input)\n",
    "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "        visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        contextual_feature = self.SequenceModeling(visual_feature)\n",
    "\n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "        prediction = self.Prediction(contextual_feature.contiguous())\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aZq80prNoXv"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "number = '0123456789'\n",
    "symbol  = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '\n",
    "chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÁÂÃÄÅÆÇÈÉÊËÍÎÑÒÓÔÕÖØÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿąęĮįıŁłŒœŠšųŽž'\n",
    "\n",
    "characters = number+ symbol + chars\n",
    "network_params = { 'input_channel': 1,\n",
    "                    'output_channel': 512,\n",
    "                    'hidden_size': 512,\n",
    "                    'num_class' : len(characters)+1\n",
    "                    }\n",
    "\n",
    "\n",
    "model = Model(**network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh43XtHT-7pw",
    "outputId": "96076fb7-7aef-4908-a3ea-c68b339772a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-26 08:31:20--  https://github.com/JaidedAI/EasyOCR/releases/download/pre-v1.1.6/latin.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/247266215/3c35f000-d635-11ea-91ba-5b9cf9318d00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201126T083120Z&X-Amz-Expires=300&X-Amz-Signature=ff554a52a2c7b6191f3c088cdbd86cc92af194a56413a744395baec35490274c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=247266215&response-content-disposition=attachment%3B%20filename%3Dlatin.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-11-26 08:31:20--  https://github-production-release-asset-2e65be.s3.amazonaws.com/247266215/3c35f000-d635-11ea-91ba-5b9cf9318d00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201126T083120Z&X-Amz-Expires=300&X-Amz-Signature=ff554a52a2c7b6191f3c088cdbd86cc92af194a56413a744395baec35490274c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=247266215&response-content-disposition=attachment%3B%20filename%3Dlatin.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.81.84\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.81.84|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 199830468 (191M) [application/octet-stream]\n",
      "Saving to: ‘latin.zip’\n",
      "\n",
      "latin.zip           100%[===================>] 190.57M  97.8MB/s    in 1.9s    \n",
      "\n",
      "2020-11-26 08:31:22 (97.8 MB/s) - ‘latin.zip’ saved [199830468/199830468]\n",
      "\n",
      "Archive:  latin.zip\n",
      "  inflating: latin.pth               \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/JaidedAI/EasyOCR/releases/download/pre-v1.1.6/latin.zip\n",
    "!unzip latin.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtFVGH44A6Ev",
    "outputId": "00920b29-3d0f-4208-ccc2-cb9211be0a01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'latin.pth'\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key[7:]\n",
    "    new_state_dict[new_key] = value\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "L9h3Wy8HRrbS",
    "outputId": "a9c88611-13be-4289-8f88-54f9e2b6bf5d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py\u001b[0m in \u001b[0;36msymbolic_fn\u001b[0;34m(g, input, output_size)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m_parse_arg\u001b[0;34m(value, desc)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     raise RuntimeError(\"Failed to export an ONNX attribute '\" + v.node().kind() +\n\u001b[0;32m---> 81\u001b[0;31m                                        \u001b[0;34m\"', since it's not constant, please try to make \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                                        \"things (e.g., kernel size) static if possible\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to export an ONNX attribute 'onnx::Gather', since it's not constant, please try to make things (e.g., kernel size) static if possible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8d0dbbdb8c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m               'output' : {1 : 'width',\n\u001b[1;32m      7\u001b[0m                           2 : 'height'}}\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ocr.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                         custom_opsets, enable_onnx_checker, use_external_data_format)\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             use_external_data_format=use_external_data_format)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference, use_new_jit_passes)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                                 \u001b[0muse_new_jit_passes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_new_jit_passes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                                 dynamic_axes=dynamic_axes)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# TODO: Don't allocate a in-memory string for the protobuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, use_new_jit_passes, dynamic_axes)\u001b[0m\n\u001b[1;32m    419\u001b[0m                             \u001b[0mfixed_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0muse_new_jit_passes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_new_jit_passes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                             dynamic_axes=dynamic_axes, input_names=input_names)\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_onnx_shape_inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, use_new_jit_passes, dynamic_axes, input_names)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mdynamic_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic_axes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_set_dynamic_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_symbolic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_symbolic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(g, n, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m    932\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributeNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msymbolic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_opset9.py\u001b[0m in \u001b[0;36msymbolic_fn\u001b[0;34m(g, input, output_size)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msym_help\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_onnx_unsupported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adaptive pooling, since output_size is not constant.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"AveragePool\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GlobalAveragePool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m_onnx_unsupported\u001b[0;34m(op_name)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_onnx_unsupported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     raise RuntimeError('Unsupported: ONNX export of operator {}. '\n\u001b[0;32m--> 187\u001b[0;31m                        'Please open a bug to request ONNX export support for the missing operator.'.format(op_name))\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported: ONNX export of operator adaptive pooling, since output_size is not constant.. Please open a bug to request ONNX export support for the missing operator."
     ]
    }
   ],
   "source": [
    "image = torch.randn(1,1,64, 1024, device='cpu')\n",
    "text = torch.randn(1, 103, device='cpu')\n",
    "dynamic_axes={'input1' :{2 : 'width',\n",
    "                         3 : 'height'},    # variable lenght axes\n",
    "            'input2' :  {1 : 'features'},\n",
    "              'output' : {1 : 'width',\n",
    "                          2 : 'height'}}\n",
    "torch.onnx.export(model, (image, text), \"ocr.onnx\", opset_version=12, export_params=True, input_names=['input1', 'input2'], output_names=['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLAra_fsU7B2"
   },
   "source": [
    "## Keras OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OMyGSx8YAUN6"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ9EN-rXV00A",
    "outputId": "7f33f744-12fe-41d4-b28b-55a8916aa5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting validators\n",
      "  Downloading validators-0.18.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/ram/anaconda3/lib/python3.8/site-packages (from validators) (4.4.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ram/anaconda3/lib/python3.8/site-packages (from validators) (1.15.0)\n",
      "Installing collected packages: validators\n",
      "Successfully installed validators-0.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip install validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lEp2L33aU2XM"
   },
   "outputs": [],
   "source": [
    "DEFAULT_BUILD_PARAMS = {\n",
    "    'height': 31,\n",
    "    'width': 200,\n",
    "    'color': False,\n",
    "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
    "    'rnn_units': (128, 128),\n",
    "    'dropout': 0.25,\n",
    "    'rnn_steps_to_discard': 2,\n",
    "    'pool_size': 2,\n",
    "    'stn': True,\n",
    "}\n",
    "\n",
    "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
    "\n",
    "PRETRAINED_WEIGHTS = {\n",
    "    'kurapan': {\n",
    "        'alphabet': DEFAULT_ALPHABET,\n",
    "        'build_params': DEFAULT_BUILD_PARAMS,\n",
    "        'weights': {\n",
    "            'notop': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
    "                'filename': 'crnn_kurapan_notop.h5',\n",
    "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
    "            },\n",
    "            'top': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
    "                'filename': 'crnn_kurapan.h5',\n",
    "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def swish(x, beta=1):\n",
    "    return x * keras.backend.sigmoid(beta * x)\n",
    "\n",
    "\n",
    "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
    "\n",
    "\n",
    "def _repeat(x, num_repeats):\n",
    "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "    x = tf.reshape(x, shape=(-1, 1))\n",
    "    x = tf.matmul(x, ones)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "def _meshgrid(height, width):\n",
    "    x_linspace = tf.linspace(-1., 1., width)\n",
    "    y_linspace = tf.linspace(-1., 1., height)\n",
    "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
    "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
    "    ones = tf.ones_like(x_coordinates)\n",
    "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "    return indices_grid\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-statements\n",
    "def _transform(inputs):\n",
    "    locnet_x, locnet_y = inputs\n",
    "    output_size = locnet_x.shape[1:]\n",
    "    batch_size = tf.shape(locnet_x)[0]\n",
    "    height = tf.shape(locnet_x)[1]\n",
    "    width = tf.shape(locnet_x)[2]\n",
    "    num_channels = tf.shape(locnet_x)[3]\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
    "    locnet_y = tf.cast(locnet_y, 'float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "    indices_grid = _meshgrid(output_height, output_width)\n",
    "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
    "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
    "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
    "\n",
    "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
    "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "    x = tf.reshape(x_s, [-1])\n",
    "    y = tf.reshape(y_s, [-1])\n",
    "\n",
    "    # Interpolate\n",
    "    height_float = tf.cast(height, dtype='float32')\n",
    "    width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "\n",
    "    x = tf.cast(x, dtype='float32')\n",
    "    y = tf.cast(y, dtype='float32')\n",
    "    x = .5 * (x + 1.0) * width_float\n",
    "    y = .5 * (y + 1.0) * height_float\n",
    "\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    max_y = tf.cast(height - 1, dtype='int32')\n",
    "    max_x = tf.cast(width - 1, dtype='int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    flat_image_dimensions = width * height\n",
    "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
    "    flat_output_dimensions = output_height * output_width\n",
    "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
    "    base_y0 = base + y0 * width\n",
    "    base_y1 = base + y1 * width\n",
    "    indices_a = base_y0 + x0\n",
    "    indices_b = base_y1 + x0\n",
    "    indices_c = base_y0 + x1\n",
    "    indices_d = base_y1 + x1\n",
    "\n",
    "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
    "    flat_image = tf.cast(flat_image, dtype='float32')\n",
    "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "    transformed_image = tf.add_n([\n",
    "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
    "        area_d * pixel_values_d\n",
    "    ])\n",
    "    # Finished interpolation\n",
    "\n",
    "    transformed_image = tf.reshape(transformed_image,\n",
    "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "def CTCDecoder():\n",
    "    def decoder(y_pred):\n",
    "        input_shape = tf.keras.backend.shape(y_pred)\n",
    "        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
    "            input_shape[1], 'float32')\n",
    "        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
    "        unpadded_shape = tf.keras.backend.shape(unpadded)\n",
    "        padded = tf.pad(unpadded,\n",
    "                        paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
    "                        constant_values=-1)\n",
    "        return padded\n",
    "\n",
    "    return tf.keras.layers.Lambda(decoder, name='decode')\n",
    "\n",
    "\n",
    "def build_model(alphabet,\n",
    "                height,\n",
    "                width,\n",
    "                color,\n",
    "                filters,\n",
    "                rnn_units,\n",
    "                dropout,\n",
    "                rnn_steps_to_discard,\n",
    "                pool_size,\n",
    "                stn=True):\n",
    "    \"\"\"Build a Keras CRNN model for character recognition.\n",
    "    Args:\n",
    "        height: The height of cropped images\n",
    "        width: The width of cropped images\n",
    "        color: Whether the inputs should be in color (RGB)\n",
    "        filters: The number of filters to use for each of the 7 convolutional layers\n",
    "        rnn_units: The number of units for each of the RNN layers\n",
    "        dropout: The dropout to use for the final layer\n",
    "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
    "        pool_size: The size of the pooling steps\n",
    "        stn: Whether to add a Spatial Transformer layer\n",
    "    \"\"\"\n",
    "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
    "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
    "    inputs = keras.layers.Input((height, width, 3 if color else 1))\n",
    "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
    "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
    "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
    "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
    "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
    "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
    "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
    "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
    "    if stn:\n",
    "        # pylint: disable=pointless-string-statement\n",
    "        \"\"\"Spatial Transformer Layer\n",
    "        Implements a spatial transformer layer as described in [1]_.\n",
    "        Borrowed from [2]_:\n",
    "        downsample_fator : float\n",
    "            A value of 1 will keep the orignal size of the image.\n",
    "            Values larger than 1 will down sample the image. Values below 1 will\n",
    "            upsample the image.\n",
    "            example image: height= 100, width = 200\n",
    "            downsample_factor = 2\n",
    "            output image will then be 50, 100\n",
    "        References\n",
    "        ----------\n",
    "        .. [1]  Spatial Transformer Networks\n",
    "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
    "                Submitted on 5 Jun 2015\n",
    "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
    "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
    "        \"\"\"\n",
    "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
    "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
    "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
    "                                       activation='relu')(stn_input_layer)\n",
    "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(6,\n",
    "                                      weights=[\n",
    "                                          np.zeros((64, 6), dtype='float32'),\n",
    "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
    "                                      ])(locnet_y)\n",
    "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
    "        x = keras.layers.Lambda(_transform,\n",
    "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
    "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
    "                                           (height // pool_size**2) * filters[-1]),\n",
    "                             name='reshape')(x)\n",
    "\n",
    "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
    "\n",
    "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_10')(x)\n",
    "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_10_back')(x)\n",
    "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
    "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_11')(rnn_1_add)\n",
    "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_11_back')(rnn_1_add)\n",
    "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
    "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
    "    x = keras.layers.Dense(len(alphabet) + 1,\n",
    "                           kernel_initializer='he_normal',\n",
    "                           activation='softmax',\n",
    "                           name='fc_12')(x)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    prediction_model = keras.models.Model(inputs=inputs, outputs=CTCDecoder()(model.output))\n",
    "    labels = keras.layers.Input(name='labels', shape=[model.output_shape[1]], dtype='float32')\n",
    "    label_length = keras.layers.Input(shape=[1])\n",
    "    input_length = keras.layers.Input(shape=[1])\n",
    "    loss = keras.layers.Lambda(lambda inputs: keras.backend.ctc_batch_cost(\n",
    "        y_true=inputs[0], y_pred=inputs[1], input_length=inputs[2], label_length=inputs[3]))(\n",
    "            [labels, model.output, input_length, label_length])\n",
    "    training_model = keras.models.Model(inputs=[model.input, labels, input_length, label_length],\n",
    "                                        outputs=loss)\n",
    "    return backbone, model, training_model, prediction_model\n",
    "\n",
    "\n",
    "class Recognizer:\n",
    "    \"\"\"A text detector using the CRNN architecture.\n",
    "    Args:\n",
    "        alphabet: The alphabet the model should recognize.\n",
    "        build_params: A dictionary of build parameters for the model.\n",
    "            See `keras_ocr.recognition.build_model` for details.\n",
    "        weights: The starting weight configuration for the model.\n",
    "        include_top: Whether to include the final classification layer in the model (set\n",
    "            to False to use a custom alphabet).\n",
    "    \"\"\"\n",
    "    def __init__(self, alphabet=None, weights='kurapan', build_params=None):\n",
    "        assert alphabet or weights, 'At least one of alphabet or weights must be provided.'\n",
    "        if weights is not None:\n",
    "            build_params = build_params or PRETRAINED_WEIGHTS[weights]['build_params']\n",
    "            alphabet = alphabet or PRETRAINED_WEIGHTS[weights]['alphabet']\n",
    "        build_params = build_params or DEFAULT_BUILD_PARAMS\n",
    "        if alphabet is None:\n",
    "            alphabet = DEFAULT_ALPHABET\n",
    "        self.alphabet = alphabet\n",
    "        self.blank_label_idx = len(alphabet)\n",
    "        self.backbone, self.model, self.training_model, self.prediction_model = build_model(\n",
    "            alphabet=alphabet, **build_params)\n",
    "        if weights is not None:\n",
    "            weights_dict = PRETRAINED_WEIGHTS[weights]\n",
    "            if alphabet == weights_dict['alphabet']:\n",
    "                self.model.load_weights(\n",
    "                    download_and_verify(url=weights_dict['weights']['top']['url'],\n",
    "                                              filename=weights_dict['weights']['top']['filename'],\n",
    "                                              sha256=weights_dict['weights']['top']['sha256']))\n",
    "            else:\n",
    "                print('Provided alphabet does not match pretrained alphabet. '\n",
    "                      'Using backbone weights only.')\n",
    "                self.backbone.load_weights(\n",
    "                    download_and_verify(url=weights_dict['weights']['notop']['url'],\n",
    "                                              filename=weights_dict['weights']['notop']['filename'],\n",
    "                                              sha256=weights_dict['weights']['notop']['sha256']))\n",
    "\n",
    "    def get_batch_generator(self, image_generator, batch_size=8, lowercase=False):\n",
    "        \"\"\"\n",
    "        Generate batches of training data from an image generator. The generator\n",
    "        should yield tuples of (image, sentence) where image contains a single\n",
    "        line of text and sentence is a string representing the contents of\n",
    "        the image. If a sample weight is desired, it can be provided as a third\n",
    "        entry in the tuple, making each tuple an (image, sentence, weight) tuple.\n",
    "        Args:\n",
    "            image_generator: An image / sentence tuple generator. The images should\n",
    "                be in color even if the OCR is setup to handle grayscale as they\n",
    "                will be converted here.\n",
    "            batch_size: How many images to generate at a time.\n",
    "            lowercase: Whether to convert all characters to lowercase before\n",
    "                encoding.\n",
    "        \"\"\"\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        if self.training_model is None:\n",
    "            raise Exception('You must first call create_training_model().')\n",
    "        max_string_length = self.training_model.input_shape[1][1]\n",
    "        while True:\n",
    "            batch = [sample for sample, _ in zip(image_generator, range(batch_size))]\n",
    "            if not self.model.input_shape[-1] == 3:\n",
    "                images = [\n",
    "                    cv2.cvtColor(sample[0], cv2.COLOR_RGB2GRAY)[..., np.newaxis] for sample in batch\n",
    "                ]\n",
    "            else:\n",
    "                images = [sample[0] for sample in batch]\n",
    "            images = np.array([image.astype('float32') / 255 for image in images])\n",
    "            sentences = [sample[1].strip() for sample in batch]\n",
    "            if lowercase:\n",
    "                sentences = [sentence.lower() for sentence in sentences]\n",
    "            for c in ''.join(sentences):\n",
    "                assert c in self.alphabet, 'Found illegal character: {}'.format(c)\n",
    "            assert all(sentences), 'Found a zero length sentence.'\n",
    "            assert all(\n",
    "                len(sentence) <= max_string_length\n",
    "                for sentence in sentences), 'A sentence is longer than this model can predict.'\n",
    "            assert all(\"  \" not in sentence for sentence in sentences), (\n",
    "                'Strings with multiple sequential spaces are not permitted. '\n",
    "                'See https://github.com/faustomorales/keras-ocr/issues/54')\n",
    "            label_length = np.array([len(sentence) for sentence in sentences])[:, np.newaxis]\n",
    "            labels = np.array([[self.alphabet.index(c)\n",
    "                                for c in sentence] + [-1] * (max_string_length - len(sentence))\n",
    "                               for sentence in sentences])\n",
    "            input_length = np.ones((batch_size, 1)) * max_string_length\n",
    "            if len(batch[0]) == 3:\n",
    "                sample_weights = np.array([sample[2] for sample in batch])\n",
    "                yield (images, labels, input_length, label_length), y, sample_weights\n",
    "            else:\n",
    "                yield (images, labels, input_length, label_length), y\n",
    "\n",
    "    def recognize(self, image):\n",
    "        \"\"\"Recognize text from a single image.\n",
    "        Args:\n",
    "            image: A pre-cropped image containing characters\n",
    "        \"\"\"\n",
    "        image = read_and_fit(filepath_or_array=image,\n",
    "                                   width=self.prediction_model.input_shape[2],\n",
    "                                   height=self.prediction_model.input_shape[1],\n",
    "                                   cval=0)\n",
    "        if self.prediction_model.input_shape[-1] == 1 and image.shape[-1] == 3:\n",
    "            # Convert color to grayscale\n",
    "            image = cv2.cvtColor(image, code=cv2.COLOR_RGB2GRAY)[..., np.newaxis]\n",
    "        image = image.astype('float32') / 255\n",
    "        return ''.join([\n",
    "            self.alphabet[idx] for idx in self.prediction_model.predict(image[np.newaxis])[0]\n",
    "            if idx not in [self.blank_label_idx, -1]\n",
    "        ])\n",
    "\n",
    "    def recognize_from_boxes(self, images, box_groups, **kwargs) -> typing.List[str]:\n",
    "        \"\"\"Recognize text from images using lists of bounding boxes.\n",
    "        Args:\n",
    "            images: A list of input images, supplied as numpy arrays with shape\n",
    "                (H, W, 3).\n",
    "            boxes: A list of groups of boxes, one for each image\n",
    "        \"\"\"\n",
    "        assert len(box_groups) == len(images), \\\n",
    "            'You must provide the same number of box groups as images.'\n",
    "        crops = []\n",
    "        start_end = []\n",
    "        for image, boxes in zip(images, box_groups):\n",
    "            image = read(image)\n",
    "            if self.prediction_model.input_shape[-1] == 1 and image.shape[-1] == 3:\n",
    "                # Convert color to grayscale\n",
    "                image = cv2.cvtColor(image, code=cv2.COLOR_RGB2GRAY)\n",
    "            for box in boxes:\n",
    "                crops.append(\n",
    "                    warpBox(image=image,\n",
    "                                  box=box,\n",
    "                                  target_height=self.model.input_shape[1],\n",
    "                                  target_width=self.model.input_shape[2]))\n",
    "            start = 0 if not start_end else start_end[-1][1]\n",
    "            start_end.append((start, start + len(boxes)))\n",
    "        if not crops:\n",
    "            return [[] for image in images]\n",
    "        X = np.float32(crops) / 255\n",
    "        if len(X.shape) == 3:\n",
    "            X = X[..., np.newaxis]\n",
    "        predictions = [\n",
    "            ''.join([self.alphabet[idx] for idx in row if idx not in [self.blank_label_idx, -1]])\n",
    "            for row in self.prediction_model.predict(X, **kwargs)\n",
    "        ]\n",
    "        return [predictions[start:end] for start, end in start_end]\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        \"\"\"Compile the training model.\"\"\"\n",
    "        if 'optimizer' not in kwargs:\n",
    "            kwargs['optimizer'] = 'RMSprop'\n",
    "        if 'loss' not in kwargs:\n",
    "            kwargs['loss'] = lambda _, y_pred: y_pred\n",
    "        self.training_model.compile(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lGCvHQjFVU0e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import typing\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "import cv2\n",
    "import imgaug\n",
    "import numpy as np\n",
    "import validators\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import geometry\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def read(filepath_or_buffer: typing.Union[str, io.BytesIO]):\n",
    "    \"\"\"Read a file into an image object\n",
    "    Args:\n",
    "        filepath_or_buffer: The path to the file, a URL, or any object\n",
    "            with a `read` method (such as `io.BytesIO`)\n",
    "    \"\"\"\n",
    "    if isinstance(filepath_or_buffer, np.ndarray):\n",
    "        return filepath_or_buffer\n",
    "    if hasattr(filepath_or_buffer, 'read'):\n",
    "        image = np.asarray(bytearray(filepath_or_buffer.read()), dtype=np.uint8)\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_UNCHANGED)\n",
    "    elif isinstance(filepath_or_buffer, str):\n",
    "        if validators.url(filepath_or_buffer):\n",
    "            return read(urllib.request.urlopen(filepath_or_buffer))\n",
    "        assert os.path.isfile(filepath_or_buffer), \\\n",
    "            'Could not find image at path: ' + filepath_or_buffer\n",
    "        image = cv2.imread(filepath_or_buffer)\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def get_rotated_width_height(box):\n",
    "    \"\"\"\n",
    "    Returns the width and height of a rotated rectangle\n",
    "    Args:\n",
    "        box: A list of four points starting in the top left\n",
    "        corner and moving clockwise.\n",
    "    \"\"\"\n",
    "    w = (spatial.distance.cdist(box[0][np.newaxis], box[1][np.newaxis], \"euclidean\") +\n",
    "         spatial.distance.cdist(box[2][np.newaxis], box[3][np.newaxis], \"euclidean\")) / 2\n",
    "    h = (spatial.distance.cdist(box[0][np.newaxis], box[3][np.newaxis], \"euclidean\") +\n",
    "         spatial.distance.cdist(box[1][np.newaxis], box[2][np.newaxis], \"euclidean\")) / 2\n",
    "    return int(w[0][0]), int(h[0][0])\n",
    "\n",
    "\n",
    "# pylint:disable=too-many-locals\n",
    "def warpBox(image,\n",
    "            box,\n",
    "            target_height=None,\n",
    "            target_width=None,\n",
    "            margin=0,\n",
    "            cval=None,\n",
    "            return_transform=False,\n",
    "            skip_rotate=False):\n",
    "    \"\"\"Warp a boxed region in an image given by a set of four points into\n",
    "    a rectangle with a specified width and height. Useful for taking crops\n",
    "    of distorted or rotated text.\n",
    "    Args:\n",
    "        image: The image from which to take the box\n",
    "        box: A list of four points starting in the top left\n",
    "            corner and moving clockwise.\n",
    "        target_height: The height of the output rectangle\n",
    "        target_width: The width of the output rectangle\n",
    "        return_transform: Whether to return the transformation\n",
    "            matrix with the image.\n",
    "    \"\"\"\n",
    "    if cval is None:\n",
    "        cval = (0, 0, 0) if len(image.shape) == 3 else 0\n",
    "    if not skip_rotate:\n",
    "        box, _ = get_rotated_box(box)\n",
    "    w, h = get_rotated_width_height(box)\n",
    "    assert (\n",
    "        (target_width is None and target_height is None)\n",
    "        or (target_width is not None and target_height is not None)), \\\n",
    "            'Either both or neither of target width and height must be provided.'\n",
    "    if target_width is None and target_height is None:\n",
    "        target_width = w\n",
    "        target_height = h\n",
    "    scale = min(target_width / w, target_height / h)\n",
    "    M = cv2.getPerspectiveTransform(src=box,\n",
    "                                    dst=np.array([[margin, margin], [scale * w - margin, margin],\n",
    "                                                  [scale * w - margin, scale * h - margin],\n",
    "                                                  [margin, scale * h - margin]]).astype('float32'))\n",
    "    crop = cv2.warpPerspective(image, M, dsize=(int(scale * w), int(scale * h)))\n",
    "    target_shape = (target_height, target_width, 3) if len(image.shape) == 3 else (target_height,\n",
    "                                                                                   target_width)\n",
    "    full = (np.zeros(target_shape) + cval).astype('uint8')\n",
    "    full[:crop.shape[0], :crop.shape[1]] = crop\n",
    "    if return_transform:\n",
    "        return full, M\n",
    "    return full\n",
    "\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def combine_line(line):\n",
    "    \"\"\"Combine a set of boxes in a line into a single bounding\n",
    "    box.\n",
    "    Args:\n",
    "        line: A list of (box, character) entries\n",
    "    Returns:\n",
    "        A (box, text) tuple\n",
    "    \"\"\"\n",
    "    text = ''.join([character if character is not None else '' for _, character in line])\n",
    "    box = np.concatenate([coords[:2] for coords, _ in line] +\n",
    "                         [np.array([coords[3], coords[2]])\n",
    "                          for coords, _ in reversed(line)]).astype('float32')\n",
    "    first_point = box[0]\n",
    "    rectangle = cv2.minAreaRect(box)\n",
    "    box = cv2.boxPoints(rectangle)\n",
    "\n",
    "    # Put the points in clockwise order\n",
    "    box = np.array(np.roll(box, -np.linalg.norm(box - first_point, axis=1).argmin(), 0))\n",
    "    return box, text\n",
    "\n",
    "\n",
    "def drawAnnotations(image, predictions, ax=None):\n",
    "    \"\"\"Draw text annotations onto image.\n",
    "    Args:\n",
    "        image: The image on which to draw\n",
    "        predictions: The predictions as provided by `pipeline.recognize`.\n",
    "        ax: A matplotlib axis on which to draw.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.imshow(drawBoxes(image=image, boxes=predictions, boxes_format='predictions'))\n",
    "    predictions = sorted(predictions, key=lambda p: p[1][:, 1].min())\n",
    "    left = []\n",
    "    right = []\n",
    "    for word, box in predictions:\n",
    "        if box[:, 0].min() < image.shape[1] / 2:\n",
    "            left.append((word, box))\n",
    "        else:\n",
    "            right.append((word, box))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    for side, group in zip(['left', 'right'], [left, right]):\n",
    "        for index, (text, box) in enumerate(group):\n",
    "            y = 1 - (index / len(group))\n",
    "            xy = box[0] / np.array([image.shape[1], image.shape[0]])\n",
    "            xy[1] = 1 - xy[1]\n",
    "            ax.annotate(s=text,\n",
    "                        xy=xy,\n",
    "                        xytext=(-0.05 if side == 'left' else 1.05, y),\n",
    "                        xycoords='axes fraction',\n",
    "                        arrowprops={\n",
    "                            'arrowstyle': '->',\n",
    "                            'color': 'r'\n",
    "                        },\n",
    "                        color='r',\n",
    "                        fontsize=14,\n",
    "                        horizontalalignment='right' if side == 'left' else 'left')\n",
    "    return ax\n",
    "\n",
    "\n",
    "def drawBoxes(image, boxes, color=(255, 0, 0), thickness=5, boxes_format='boxes'):\n",
    "    \"\"\"Draw boxes onto an image.\n",
    "    Args:\n",
    "        image: The image on which to draw the boxes.\n",
    "        boxes: The boxes to draw.\n",
    "        color: The color for each box.\n",
    "        thickness: The thickness for each box.\n",
    "        boxes_format: The format used for providing the boxes. Options are\n",
    "            \"boxes\" which indicates an array with shape(N, 4, 2) where N is the\n",
    "            number of boxes and each box is a list of four points) as provided\n",
    "            by `keras_ocr.detection.Detector.detect`, \"lines\" (a list of\n",
    "            lines where each line itself is a list of (box, character) tuples) as\n",
    "            provided by `keras_ocr.data_generation.get_image_generator`,\n",
    "            or \"predictions\" where boxes is by itself a list of (word, box) tuples\n",
    "            as provided by `keras_ocr.pipeline.Pipeline.recognize` or\n",
    "            `keras_ocr.recognition.Recognizer.recognize_from_boxes`.\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return image\n",
    "    canvas = image.copy()\n",
    "    if boxes_format == 'lines':\n",
    "        revised_boxes = []\n",
    "        for line in boxes:\n",
    "            for box, _ in line:\n",
    "                revised_boxes.append(box)\n",
    "        boxes = revised_boxes\n",
    "    if boxes_format == 'predictions':\n",
    "        revised_boxes = []\n",
    "        for _, box in boxes:\n",
    "            revised_boxes.append(box)\n",
    "        boxes = revised_boxes\n",
    "    for box in boxes:\n",
    "        cv2.polylines(img=canvas,\n",
    "                      pts=box[np.newaxis].astype('int32'),\n",
    "                      color=color,\n",
    "                      thickness=thickness,\n",
    "                      isClosed=True)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def adjust_boxes(boxes, boxes_format='boxes', scale=1):\n",
    "    \"\"\"Adjust boxes using a given scale and offset.\n",
    "    Args:\n",
    "        boxes: The boxes to adjust\n",
    "        boxes_format: The format for the boxes. See the `drawBoxes` function\n",
    "            for an explanation on the options.\n",
    "        scale: The scale to apply\n",
    "    \"\"\"\n",
    "    if scale == 1:\n",
    "        return boxes\n",
    "    if boxes_format == 'boxes':\n",
    "        return np.array(boxes) * scale\n",
    "    if boxes_format == 'lines':\n",
    "        return [[(np.array(box) * scale, character) for box, character in line] for line in boxes]\n",
    "    if boxes_format == 'predictions':\n",
    "        return [(word, np.array(box) * scale) for word, box in boxes]\n",
    "    raise NotImplementedError(f'Unsupported boxes format: {boxes_format}')\n",
    "\n",
    "\n",
    "def augment(boxes,\n",
    "            augmenter: imgaug.augmenters.meta.Augmenter,\n",
    "            image=None,\n",
    "            boxes_format='boxes',\n",
    "            image_shape=None,\n",
    "            area_threshold=0.5,\n",
    "            min_area=None):\n",
    "    \"\"\"Augment an image and associated boxes together.\n",
    "    Args:\n",
    "        image: The image which we wish to apply the augmentation.\n",
    "        boxes: The boxes that will be augmented together with the image\n",
    "        boxes_format: The format for the boxes. See the `drawBoxes` function\n",
    "            for an explanation on the options.\n",
    "        image_shape: The shape of the input image if no image will be provided.\n",
    "        area_threshold: Fraction of bounding box that we require to be\n",
    "            in augmented image to include it.\n",
    "        min_area: The minimum area for a character to be included.\n",
    "    \"\"\"\n",
    "    if image is None and image_shape is None:\n",
    "        raise ValueError('One of \"image\" or \"image_shape\" must be provided.')\n",
    "    augmenter = augmenter.to_deterministic()\n",
    "\n",
    "    if image is not None:\n",
    "        image_augmented = augmenter(image=image)\n",
    "        image_shape = image.shape[:2]\n",
    "        image_augmented_shape = image_augmented.shape[:2]\n",
    "    else:\n",
    "        image_augmented = None\n",
    "        width_augmented, height_augmented = augmenter.augment_keypoints(\n",
    "            imgaug.KeypointsOnImage.from_xy_array(xy=[[image_shape[1], image_shape[0]]],\n",
    "                                                  shape=image_shape)).to_xy_array()[0]\n",
    "        image_augmented_shape = (height_augmented, width_augmented)\n",
    "\n",
    "    def box_inside_image(box):\n",
    "        area_before = cv2.contourArea(np.int32(box)[:, np.newaxis, :])\n",
    "        if area_before == 0:\n",
    "            return False, box\n",
    "        clipped = box.copy()\n",
    "        clipped[:, 0] = clipped[:, 0].clip(0, image_augmented_shape[1])\n",
    "        clipped[:, 1] = clipped[:, 1].clip(0, image_augmented_shape[0])\n",
    "        area_after = cv2.contourArea(np.int32(clipped)[:, np.newaxis, :])\n",
    "        return ((area_after / area_before) >= area_threshold) and (min_area is None or\n",
    "                                                                   area_after > min_area), clipped\n",
    "\n",
    "    def augment_box(box):\n",
    "        return augmenter.augment_keypoints(\n",
    "            imgaug.KeypointsOnImage.from_xy_array(box, shape=image_shape)).to_xy_array()\n",
    "\n",
    "    if boxes_format == 'boxes':\n",
    "        boxes_augmented = [\n",
    "            box for inside, box in [box_inside_image(box) for box in map(augment_box, boxes)]\n",
    "            if inside\n",
    "        ]\n",
    "    elif boxes_format == 'lines':\n",
    "        boxes_augmented = [[(augment_box(box), character) for box, character in line]\n",
    "                           for line in boxes]\n",
    "        boxes_augmented = [[(box, character)\n",
    "                            for (inside, box), character in [(box_inside_image(box), character)\n",
    "                                                             for box, character in line] if inside]\n",
    "                           for line in boxes_augmented]\n",
    "        # Sometimes all the characters in a line are removed.\n",
    "        boxes_augmented = [line for line in boxes_augmented if line]\n",
    "    elif boxes_format == 'predictions':\n",
    "        boxes_augmented = [(word, augment_box(box)) for word, box in boxes]\n",
    "        boxes_augmented = [(word, box) for word, (inside, box) in [(word, box_inside_image(box))\n",
    "                                                                   for word, box in boxes_augmented]\n",
    "                           if inside]\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported boxes format: {boxes_format}')\n",
    "    return image_augmented, boxes_augmented\n",
    "\n",
    "\n",
    "def pad(image, width: int, height: int, cval: int = 255):\n",
    "    \"\"\"Pad an image to a desired size. Raises an exception if image\n",
    "    is larger than desired size.\n",
    "    Args:\n",
    "        image: The input image\n",
    "        width: The output width\n",
    "        height: The output height\n",
    "        cval: The value to use for filling the image.\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        output_shape = (height, width, image.shape[-1])\n",
    "    else:\n",
    "        output_shape = (height, width)\n",
    "    assert height >= output_shape[0], 'Input height must be less than output height.'\n",
    "    assert width >= output_shape[1], 'Input width must be less than output width.'\n",
    "    padded = np.zeros(output_shape, dtype=image.dtype) + cval\n",
    "    padded[:image.shape[0], :image.shape[1]] = image\n",
    "    return padded\n",
    "\n",
    "\n",
    "def resize_image(image, max_scale, max_size):\n",
    "    \"\"\"Obtain the optimal resized image subject to a maximum scale\n",
    "    and maximum size.\n",
    "    Args:\n",
    "        image: The input image\n",
    "        max_scale: The maximum scale to apply\n",
    "        max_size: The maximum size to return\n",
    "    \"\"\"\n",
    "    if max(image.shape) * max_scale > max_size:\n",
    "        # We are constrained by the maximum size\n",
    "        scale = max_size / max(image.shape)\n",
    "    else:\n",
    "        # We are contrained by scale\n",
    "        scale = max_scale\n",
    "    return cv2.resize(image,\n",
    "                      dsize=(int(image.shape[1] * scale), int(image.shape[0] * scale))), scale\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-arguments\n",
    "def fit(image, width: int, height: int, cval: int = 255, mode='letterbox', return_scale=False):\n",
    "    \"\"\"Obtain a new image, fit to the specified size.\n",
    "    Args:\n",
    "        image: The input image\n",
    "        width: The new width\n",
    "        height: The new height\n",
    "        cval: The constant value to use to fill the remaining areas of\n",
    "            the image\n",
    "        return_scale: Whether to return the scale used for the image\n",
    "    Returns:\n",
    "        The new image\n",
    "    \"\"\"\n",
    "    fitted = None\n",
    "    x_scale = width / image.shape[1]\n",
    "    y_scale = height / image.shape[0]\n",
    "    if x_scale == 1 and y_scale == 1:\n",
    "        fitted = image\n",
    "        scale = 1\n",
    "    elif (x_scale <= y_scale and mode == 'letterbox') or (x_scale >= y_scale and mode == 'crop'):\n",
    "        scale = width / image.shape[1]\n",
    "        resize_width = width\n",
    "        resize_height = (width / image.shape[1]) * image.shape[0]\n",
    "    else:\n",
    "        scale = height / image.shape[0]\n",
    "        resize_height = height\n",
    "        resize_width = scale * image.shape[1]\n",
    "    if fitted is None:\n",
    "        resize_width, resize_height = map(int, [resize_width, resize_height])\n",
    "        if mode == 'letterbox':\n",
    "            fitted = np.zeros((height, width, 3), dtype='uint8') + cval\n",
    "            image = cv2.resize(image, dsize=(resize_width, resize_height))\n",
    "            fitted[:image.shape[0], :image.shape[1]] = image[:height, :width]\n",
    "        elif mode == 'crop':\n",
    "            image = cv2.resize(image, dsize=(resize_width, resize_height))\n",
    "            fitted = image[:height, :width]\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unsupported mode: {mode}')\n",
    "    if not return_scale:\n",
    "        return fitted\n",
    "    return fitted, scale\n",
    "\n",
    "\n",
    "def read_and_fit(filepath_or_array: typing.Union[str, np.ndarray],\n",
    "                 width: int,\n",
    "                 height: int,\n",
    "                 cval: int = 255,\n",
    "                 mode='letterbox'):\n",
    "    \"\"\"Read an image from disk and fit to the specified size.\n",
    "    Args:\n",
    "        filepath: The path to the image or numpy array of shape HxWx3\n",
    "        width: The new width\n",
    "        height: The new height\n",
    "        cval: The constant value to use to fill the remaining areas of\n",
    "            the image\n",
    "        mode: The mode to pass to \"fit\" (crop or letterbox)\n",
    "    Returns:\n",
    "        The new image\n",
    "    \"\"\"\n",
    "    image = read(filepath_or_array) if isinstance(filepath_or_array, str) else filepath_or_array\n",
    "    image = fit(image=image, width=width, height=height, cval=cval, mode=mode)\n",
    "    return image\n",
    "\n",
    "\n",
    "def sha256sum(filename):\n",
    "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    b = bytearray(128 * 1024)\n",
    "    mv = memoryview(b)\n",
    "    with open(filename, 'rb', buffering=0) as f:\n",
    "        for n in iter(lambda: f.readinto(mv), 0):\n",
    "            h.update(mv[:n])\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def get_default_cache_dir():\n",
    "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
    "                                                                                 '.keras-ocr')))\n",
    "\n",
    "\n",
    "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
    "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
    "    hash.\n",
    "    Args:\n",
    "        url: The file to download\n",
    "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
    "            matches, we don't download it again.\n",
    "        cache_dir: The directory in which to cache the file. The default is\n",
    "            `~/.keras-ocr`.\n",
    "        verbose: Whether to log progress\n",
    "        filename: The filename to use for the file. By default, the filename is\n",
    "            derived from the URL.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = get_default_cache_dir()\n",
    "    if filename is None:\n",
    "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
    "    if verbose:\n",
    "        print('Looking for ' + filepath)\n",
    "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
    "        if verbose:\n",
    "            print('Downloading ' + filepath)\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def get_rotated_box(\n",
    "    points\n",
    ") -> typing.Tuple[typing.Tuple[float, float], typing.Tuple[float, float], typing.Tuple[\n",
    "        float, float], typing.Tuple[float, float], float]:\n",
    "    \"\"\"Obtain the parameters of a rotated box.\n",
    "    Returns:\n",
    "        The vertices of the rotated box in top-left,\n",
    "        top-right, bottom-right, bottom-left order along\n",
    "        with the angle of rotation about the bottom left corner.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mp = geometry.MultiPoint(points=points)\n",
    "        pts = np.array(list(zip(*mp.minimum_rotated_rectangle.exterior.xy)))[:-1]  # noqa: E501\n",
    "    except AttributeError:\n",
    "        # There weren't enough points for the minimum rotated rectangle function\n",
    "        pts = points\n",
    "    # The code below is taken from\n",
    "    # https://github.com/jrosebr1/imutils/blob/master/imutils/perspective.py\n",
    "\n",
    "    # sort the points based on their x-coordinates\n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\n",
    "    # grab the left-most and right-most points from the sorted\n",
    "    # x-roodinate points\n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "\n",
    "    # now, sort the left-most coordinates according to their\n",
    "    # y-coordinates so we can grab the top-left and bottom-left\n",
    "    # points, respectively\n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "\n",
    "    # now that we have the top-left coordinate, use it as an\n",
    "    # anchor to calculate the Euclidean distance between the\n",
    "    # top-left and right-most points; by the Pythagorean\n",
    "    # theorem, the point with the largest distance will be\n",
    "    # our bottom-right point\n",
    "    D = spatial.distance.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\n",
    "    # return the coordinates in top-left, top-right,\n",
    "    # bottom-right, and bottom-left order\n",
    "    pts = np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "    rotation = np.arctan((tl[0] - bl[0]) / (tl[1] - bl[1]))\n",
    "    return pts, rotation\n",
    "\n",
    "\n",
    "def fix_line(line):\n",
    "    \"\"\"Given a list of (box, character) tuples, return a revised\n",
    "    line with a consistent ordering of left-to-right or top-to-bottom,\n",
    "    with each box provided with (top-left, top-right, bottom-right, bottom-left)\n",
    "    ordering.\n",
    "    Returns:\n",
    "        A tuple that is the fixed line as well as a string indicating\n",
    "        whether the line is horizontal or vertical.\n",
    "    \"\"\"\n",
    "    line = [(get_rotated_box(box)[0], character) for box, character in line]\n",
    "    centers = np.array([box.mean(axis=0) for box, _ in line])\n",
    "    sortedx = centers[:, 0].argsort()\n",
    "    sortedy = centers[:, 1].argsort()\n",
    "    if np.diff(centers[sortedy][:, 1]).sum() > np.diff(centers[sortedx][:, 0]).sum():\n",
    "        return [line[idx] for idx in sortedy], 'vertical'\n",
    "    return [line[idx] for idx in sortedx], 'horizontal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eY24rnbMVjtI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Pipeline:\n",
    "    \"\"\"A wrapper for a combination of detector and recognizer.\n",
    "    Args:\n",
    "        detector: The detector to use\n",
    "        recognizer: The recognizer to use\n",
    "        scale: The scale factor to apply to input images\n",
    "        max_size: The maximum single-side dimension of images for\n",
    "            inference.\n",
    "    \"\"\"\n",
    "    def __init__(self, detector=None, recognizer=None, scale=2, max_size=2048):\n",
    "        if detector is None:\n",
    "            detector = detection.Detector()\n",
    "        if recognizer is None:\n",
    "            recognizer = recognition.Recognizer()\n",
    "        self.scale = scale\n",
    "        self.detector = detector\n",
    "        self.recognizer = recognizer\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def recognize(self, images, detection_kwargs=None, recognition_kwargs=None):\n",
    "        \"\"\"Run the pipeline on one or multiples images.\n",
    "        Args:\n",
    "            images: The images to parse (can be a list of actual images or a list of filepaths)\n",
    "            detection_kwargs: Arguments to pass to the detector call\n",
    "            recognition_kwargs: Arguments to pass to the recognizer call\n",
    "        Returns:\n",
    "            A list of lists of (text, box) tuples.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure we have an image array to start with.\n",
    "        if not isinstance(images, np.ndarray):\n",
    "            images = [tools.read(image) for image in images]\n",
    "        # This turns images into (image, scale) tuples temporarily\n",
    "        images = [\n",
    "            tools.resize_image(image, max_scale=self.scale, max_size=self.max_size)\n",
    "            for image in images\n",
    "        ]\n",
    "        max_height, max_width = np.array([image.shape[:2] for image, scale in images]).max(axis=0)\n",
    "        scales = [scale for _, scale in images]\n",
    "        images = np.array(\n",
    "            [tools.pad(image, width=max_width, height=max_height) for image, _ in images])\n",
    "        if detection_kwargs is None:\n",
    "            detection_kwargs = {}\n",
    "        if recognition_kwargs is None:\n",
    "            recognition_kwargs = {}\n",
    "        box_groups = self.detector.detect(images=images, **detection_kwargs)\n",
    "        prediction_groups = self.recognizer.recognize_from_boxes(images=images,\n",
    "                                                                 box_groups=box_groups,\n",
    "                                                                 **recognition_kwargs)\n",
    "        box_groups = [\n",
    "            adjust_boxes(boxes=boxes, boxes_format='boxes', scale=1 /\n",
    "                               scale) if scale != 1 else boxes\n",
    "            for boxes, scale in zip(box_groups, scales)\n",
    "        ]\n",
    "        return [\n",
    "            list(zip(predictions, boxes))\n",
    "            for predictions, boxes in zip(prediction_groups, box_groups)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/ram/.keras-ocr/crnn_kurapan.h5\n",
      "Downloading /home/ram/.keras-ocr/crnn_kurapan.h5\n"
     ]
    }
   ],
   "source": [
    "recognizer = Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "R5Phgv2-X_fH",
    "outputId": "d4e4e150-4d73-4c2d-8a06-6b227a601ba0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7aeaacbcffae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c80025e1e3c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, detector, recognizer, scale, max_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecognizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mrecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detection' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2JfJLAOYV6OX"
   },
   "outputs": [],
   "source": [
    "!cp /root/.keras-ocr/crnn_kurapan.h5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AHS7bTMSWXi5"
   },
   "outputs": [],
   "source": [
    "output = recognizer.recognize('/home/ram/Projects/OCR/deep-text-recognition-benchmark/demo_image/demo_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nYXrmXuoXMow"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(recognizer.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Kfxk6Sn2XOGa"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s09XWxYsZg_x",
    "outputId": "656a3b09-74f0-433e-aa20-fbadafb28d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps4x75ej8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps4x75ej8/assets\n"
     ]
    }
   ],
   "source": [
    "tf_lite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evpo0xziZiid"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OCR_TFLITE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXXLK40FW1fc"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml8RTXFZxPkd"
   },
   "source": [
    "## SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i3X7uUDbxCXD",
    "outputId": "b7cd3c62-ee6e-4aa4-f122-efb02b0fea1a"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4o-1GNYVxWUw",
    "outputId": "38eefccb-098c-4d37-b252-b39543f48d35"
   },
   "outputs": [],
   "source": [
    "!pip install validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5Mme4MUCxVM8",
    "outputId": "55e8e0ca-c98b-441f-9953-026fbd33c41b"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdv7jaX-W7fd"
   },
   "source": [
    "**All of the code required to build the model and load the weights are taken from this [repository](https://github.com/faustomorales/keras-ocr) including the pretrained weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwgbqcnBxnc9"
   },
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duqhKKqbxpc9"
   },
   "outputs": [],
   "source": [
    "DEFAULT_BUILD_PARAMS = {\n",
    "    'height': 31,\n",
    "    'width': 200,\n",
    "    'color': False,\n",
    "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
    "    'rnn_units': (128, 128),\n",
    "    'dropout': 0.25,\n",
    "    'rnn_steps_to_discard': 2,\n",
    "    'pool_size': 2,\n",
    "    'stn': True,\n",
    "}\n",
    "\n",
    "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
    "\n",
    "PRETRAINED_WEIGHTS = {\n",
    "    'kurapan': {\n",
    "        'alphabet': DEFAULT_ALPHABET,\n",
    "        'build_params': DEFAULT_BUILD_PARAMS,\n",
    "        'weights': {\n",
    "            'notop': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
    "                'filename': 'crnn_kurapan_notop.h5',\n",
    "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
    "            },\n",
    "            'top': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
    "                'filename': 'crnn_kurapan.h5',\n",
    "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH9oviLCzOI6"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkP9-8YZzTWh"
   },
   "outputs": [],
   "source": [
    "def swish(x, beta=1):\n",
    "    return x * keras.backend.sigmoid(beta * x)\n",
    "\n",
    "\n",
    "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
    "\n",
    "\n",
    "def _repeat(x, num_repeats):\n",
    "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "    x = tf.reshape(x, shape=(-1, 1))\n",
    "    x = tf.matmul(x, ones)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "def _meshgrid(height, width):\n",
    "    x_linspace = tf.linspace(-1., 1., width)\n",
    "    y_linspace = tf.linspace(-1., 1., height)\n",
    "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
    "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
    "    ones = tf.ones_like(x_coordinates)\n",
    "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "    return indices_grid\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-statements\n",
    "def _transform(inputs):\n",
    "    locnet_x, locnet_y = inputs\n",
    "    output_size = locnet_x.shape[1:]\n",
    "    batch_size = tf.shape(locnet_x)[0]\n",
    "    height = tf.shape(locnet_x)[1]\n",
    "    width = tf.shape(locnet_x)[2]\n",
    "    num_channels = tf.shape(locnet_x)[3]\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
    "    locnet_y = tf.cast(locnet_y, 'float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "    indices_grid = _meshgrid(output_height, output_width)\n",
    "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
    "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
    "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
    "\n",
    "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
    "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "    x = tf.reshape(x_s, [-1])\n",
    "    y = tf.reshape(y_s, [-1])\n",
    "\n",
    "    # Interpolate\n",
    "    height_float = tf.cast(height, dtype='float32')\n",
    "    width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "\n",
    "    x = tf.cast(x, dtype='float32')\n",
    "    y = tf.cast(y, dtype='float32')\n",
    "    x = .5 * (x + 1.0) * width_float\n",
    "    y = .5 * (y + 1.0) * height_float\n",
    "\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    max_y = tf.cast(height - 1, dtype='int32')\n",
    "    max_x = tf.cast(width - 1, dtype='int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    flat_image_dimensions = width * height\n",
    "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
    "    flat_output_dimensions = output_height * output_width\n",
    "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
    "    base_y0 = base + y0 * width\n",
    "    base_y1 = base + y1 * width\n",
    "    indices_a = base_y0 + x0\n",
    "    indices_b = base_y1 + x0\n",
    "    indices_c = base_y0 + x1\n",
    "    indices_d = base_y1 + x1\n",
    "\n",
    "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
    "    flat_image = tf.cast(flat_image, dtype='float32')\n",
    "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "    transformed_image = tf.add_n([\n",
    "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
    "        area_d * pixel_values_d\n",
    "    ])\n",
    "    # Finished interpolation\n",
    "\n",
    "    transformed_image = tf.reshape(transformed_image,\n",
    "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALRK3SR1x8JH"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaRONZgExrQL"
   },
   "outputs": [],
   "source": [
    "def build_model(alphabet,\n",
    "                height,\n",
    "                width,\n",
    "                color,\n",
    "                filters,\n",
    "                rnn_units,\n",
    "                dropout,\n",
    "                rnn_steps_to_discard,\n",
    "                pool_size,\n",
    "                stn=True):\n",
    "    \"\"\"Build a Keras CRNN model for character recognition.\n",
    "    Args:\n",
    "        height: The height of cropped images\n",
    "        width: The width of cropped images\n",
    "        color: Whether the inputs should be in color (RGB)\n",
    "        filters: The number of filters to use for each of the 7 convolutional layers\n",
    "        rnn_units: The number of units for each of the RNN layers\n",
    "        dropout: The dropout to use for the final layer\n",
    "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
    "        pool_size: The size of the pooling steps\n",
    "        stn: Whether to add a Spatial Transformer layer\n",
    "    \"\"\"\n",
    "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
    "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
    "    inputs = keras.layers.Input((height, width, 3 if color else 1), name='input', batch_size=1)\n",
    "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
    "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
    "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
    "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
    "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
    "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
    "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
    "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
    "    if stn:\n",
    "        # pylint: disable=pointless-string-statement\n",
    "        \"\"\"Spatial Transformer Layer\n",
    "        Implements a spatial transformer layer as described in [1]_.\n",
    "        Borrowed from [2]_:\n",
    "        downsample_fator : float\n",
    "            A value of 1 will keep the orignal size of the image.\n",
    "            Values larger than 1 will down sample the image. Values below 1 will\n",
    "            upsample the image.\n",
    "            example image: height= 100, width = 200\n",
    "            downsample_factor = 2\n",
    "            output image will then be 50, 100\n",
    "        References\n",
    "        ----------\n",
    "        .. [1]  Spatial Transformer Networks\n",
    "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
    "                Submitted on 5 Jun 2015\n",
    "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
    "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
    "        \"\"\"\n",
    "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
    "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
    "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
    "                                       activation='relu')(stn_input_layer)\n",
    "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(6,\n",
    "                                      weights=[\n",
    "                                          np.zeros((64, 6), dtype='float32'),\n",
    "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
    "                                      ])(locnet_y)\n",
    "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
    "        x = keras.layers.Lambda(_transform,\n",
    "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
    "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
    "                                           (height // pool_size**2) * filters[-1]),\n",
    "                             name='reshape')(x)\n",
    "\n",
    "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
    "\n",
    "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_10')(x)\n",
    "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_10_back')(x)\n",
    "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
    "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_11')(rnn_1_add)\n",
    "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_11_back')(rnn_1_add)\n",
    "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
    "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
    "    x = keras.layers.Dense(len(alphabet) + 1,\n",
    "                           kernel_initializer='he_normal',\n",
    "                           activation='softmax',\n",
    "                           name='fc_12')(x)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "#     prediction_model = keras.models.Model(inputs=inputs, outputs=CTCDecoder()(model.output))\n",
    "#     labels = keras.layers.Input(name='labels', shape=[model.output_shape[1]], dtype='float32')\n",
    "#     label_length = keras.layers.Input(shape=[1])\n",
    "#     input_length = keras.layers.Input(shape=[1])\n",
    "#     loss = keras.layers.Lambda(lambda inputs: keras.backend.ctc_batch_cost(\n",
    "#         y_true=inputs[0], y_pred=inputs[1], input_length=inputs[2], label_length=inputs[3]))(\n",
    "#             [labels, model.output, input_length, label_length])\n",
    "#     training_model = keras.models.Model(inputs=[model.input, labels, input_length, label_length],\n",
    "#                                         outputs=loss)\n",
    "    # We are commenting the above lines because CTC Decoder here is not supported in TFLite hence we are\n",
    "    # discarding the CTC Decoder Portion model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPBjgj6gyECQ",
    "outputId": "a1e5e117-7dea-43c1-c866-61cd9fbae3b2"
   },
   "outputs": [],
   "source": [
    "build_params = DEFAULT_BUILD_PARAMS\n",
    "alphabets = DEFAULT_ALPHABET\n",
    "blank_index = len(alphabets)\n",
    "\n",
    "# While building the model we are not including the CTC Decoder as it is not convertable to TFLite\n",
    "model = build_model(alphabet=alphabets, **build_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLVl95alzwI8"
   },
   "source": [
    "## Download and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmBRQeL90HoQ"
   },
   "outputs": [],
   "source": [
    "def get_default_cache_dir():\n",
    "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
    "                                                                              '.keras-ocr')))\n",
    "def sha256sum(filename):\n",
    "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    b = bytearray(128 * 1024)\n",
    "    mv = memoryview(b)\n",
    "    with open(filename, 'rb', buffering=0) as f:\n",
    "        for n in iter(lambda: f.readinto(mv), 0):\n",
    "            h.update(mv[:n])\n",
    "    return h.hexdigest()\n",
    "\n",
    "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
    "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
    "    hash.\n",
    "    Args:\n",
    "        url: The file to download\n",
    "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
    "            matches, we don't download it again.\n",
    "        cache_dir: The directory in which to cache the file. The default is\n",
    "            `~/.keras-ocr`.\n",
    "        verbose: Whether to log progress\n",
    "        filename: The filename to use for the file. By default, the filename is\n",
    "            derived from the URL.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = get_default_cache_dir()\n",
    "    if filename is None:\n",
    "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
    "    if verbose:\n",
    "        print('Looking for ' + filepath)\n",
    "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
    "        if verbose:\n",
    "            print('Downloading ' + filepath)\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5lNpYwczLuO",
    "outputId": "6308d5d8-1cd8-4985-ed90-b6f712abe6e1"
   },
   "outputs": [],
   "source": [
    "weights_dict = PRETRAINED_WEIGHTS['kurapan']\n",
    "\n",
    "model.load_weights(download_and_verify(url=weights_dict['weights']['top']['url'],\n",
    "                                       filename=weights_dict['weights']['top']['filename'],\n",
    "                                       sha256=weights_dict['weights']['top']['sha256']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czQXrbwF1rWO"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUyx8EAV0IjX",
    "outputId": "a8290d99-75dc-46f2-bad3-3fd9bc92d0ad"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dg50iNc106w"
   },
   "source": [
    "## Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6K9tv0u1umk"
   },
   "outputs": [],
   "source": [
    "def convert_tflite(quantization):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if quantization == 'float16':\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    tf_lite_model = converter.convert()\n",
    "    open(f'ocr_{quantization}.tflite', 'wb').write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtwNAQp72EHR"
   },
   "source": [
    "**Note** : Support for CTC Decoder is not available in TFLite yet. So while converting we removed CTCDecoder in model part. We need to run Decoder from the output of the model. \n",
    "\n",
    "Refer to this [issue](https://github.com/tensorflow/tensorflow/issues/33494) regarding CTC decoder support in TFLite. \n",
    "\n",
    "This is the code for [CTC DECODER](https://colab.research.google.com/github/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb#scrollTo=_rdJyCXo2Xzs). By default it is greedy Decoder we can also use Beam Search Decoder by specifying the parameter in the `ctc_decode` function.\n",
    "\n",
    "FYI: I am also working on converting Beam Search CTC Decoder to low level language so that we can port entire OCR and use it as offline application combinedly with EAST/CRAFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cnif7h6Zzfj",
    "outputId": "fc209dad-d07f-474f-ad0f-8720bb81bc1f"
   },
   "outputs": [],
   "source": [
    "quantization = 'dr' #@param [\"dr\", \"float16\"]\n",
    "convert_tflite(quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etAxCMGMZ9Dk",
    "outputId": "20d86156-b7b3-4ab8-a9b2-c94736039de5"
   },
   "outputs": [],
   "source": [
    "!du -sh ocr_dr.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoJ4Q6EaZ9Ft",
    "outputId": "00f03704-066c-4f32-ef11-146531a7c556"
   },
   "outputs": [],
   "source": [
    "quantization = 'float16' #@param [\"dr\", \"float16\"]\n",
    "convert_tflite(quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tERPNIWDaJC-",
    "outputId": "317c6d63-c0eb-495a-9ca6-df5a1efb97ca"
   },
   "outputs": [],
   "source": [
    "!du -sh ocr_float16.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rdJyCXo2Xzs"
   },
   "source": [
    "## TFLite Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPC_SGpy2vQX"
   },
   "source": [
    "#### CTC Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7mfepcG2pKN"
   },
   "outputs": [],
   "source": [
    "# Code for CTC Decoder \n",
    "\n",
    "def decoder(y_pred):\n",
    "    input_shape = tf.keras.backend.shape(y_pred)\n",
    "    input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
    "        input_shape[1], 'float32')\n",
    "    # You can turn on beam search decoding using greedy=False and also play with beam_width parameter.\n",
    "    unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
    "    unpadded_shape = tf.keras.backend.shape(unpadded)\n",
    "    padded = tf.pad(unpadded,\n",
    "                    paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
    "                    constant_values=-1)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQqZgvQz2yjl"
   },
   "source": [
    "#### TFLite model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lH2dGFydW-8C",
    "outputId": "416412c5-c173-43b5-ee36-36b9f9210613"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/tulasiram58827/ocr_tflite/raw/main/images/demo_1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtm2RNd91-vB"
   },
   "outputs": [],
   "source": [
    "def run_tflite_model(image_path, quantization):\n",
    "    input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    input_data = cv2.resize(input_data, (200, 31))\n",
    "    input_data = input_data[np.newaxis]\n",
    "    input_data = np.expand_dims(input_data, 3)\n",
    "    input_data = input_data.astype('float32')/255\n",
    "    path = f'ocr_{quantization}.tflite'\n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape']\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "QfQDiPXM2tBz",
    "outputId": "88d29692-8190-4506-c262-25d4b8ac6fff"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "# Running Integer Quantization\n",
    "image_path = '/content/demo_1.png'\n",
    "tflite_output = run_tflite_model(image_path, 'dr')\n",
    "# Running decoder on TFLite Output\n",
    "decoded = decoder(tflite_output)\n",
    "final_output = \"\".join(alphabets[index] for index in decoded[0] if index not in [blank_index, -1])\n",
    "print(final_output)\n",
    "cv2_imshow(cv2.imread(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "mu-d4ESFbiR9",
    "outputId": "4168e4d0-b9e9-4846-9033-687e457913ba"
   },
   "outputs": [],
   "source": [
    "# Running Float16 Quantization\n",
    "tflite_output = run_tflite_model(image_path, 'float16')\n",
    "# Running decoder on TFLite Output\n",
    "decoded = decoder(tflite_output)\n",
    "final_output = \"\".join(alphabets[index] for index in decoded[0] if index not in [blank_index, -1])\n",
    "print(final_output)\n",
    "cv2_imshow(cv2.imread(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjiIkzxO6oJ4"
   },
   "source": [
    "## Dynamic Range Model benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWRocpH0clnq"
   },
   "source": [
    "**Inference Time** : 0.2sec\n",
    "\n",
    "**Memory FootPrint** : 46.38MB\n",
    "\n",
    "**Model Size** : 8.5MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlrZl1GT6soU"
   },
   "source": [
    "## Float16 benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0i-uMQFdUD3"
   },
   "source": [
    "**Inference** : 0.76sec\n",
    "\n",
    "**Memory FootPrint** : 128MB\n",
    "\n",
    "**Model Size** : 17MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmnnRktG28CX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of KERAS_OCR_TFLITE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
